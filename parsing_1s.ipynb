{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "707a15a6-5132-4d1c-880c-804e9cdb5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "import os\n",
    "from dotenv import load_dotenv # для чтения файла .env \n",
    "\n",
    "import base64 # для декодирования: сначала кодируем в UTF-8, затем в Base64\n",
    "\n",
    "from requests import Session\n",
    "# requests качает → копаетсяBeautifulSoup \n",
    "import requests \n",
    "#from bs4 import BeautifulSoup # ищет теги <div> <span> и тд для Веб-сайт / грязный HTML\n",
    "    #BeautifulSoup(response.text, 'html.parser') Для HTML\n",
    "    #BeautifulSoup(response.text, 'xml') Для XML (нужен lxml)\n",
    "    # поиск JSON в теге <script> внутри HTML-страницы - BeautifulSoup + json.loads()\n",
    "            # HTML-код:\n",
    "        #<script id=\"app-data\">\n",
    "        #{\"user\": \"Ivan\", \"orders\": [1, 2, 3]}\n",
    "        #</script>\n",
    "            #поиск внутри HTML-код\n",
    "            # 1. BeautifulSoup — чтобы найти блок\n",
    "        #soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        #script = soup.find('script', id='app-data')\n",
    "            # 2. json.loads — чтобы распарсить\n",
    "        #data = json.loads(script.text)\n",
    "from xml.etree import ElementTree as ET # обработка xml для SAP/1С/SOAP/чистый XML имеет четкую структуру\n",
    "import json # обработка json # data = response.json() # Или, если нужно вручную: import json data = json.loads(response.text)\n",
    "from requests.auth import HTTPBasicAuth # базовый вариант аутентификации логин:пароль\n",
    "#from requests_ntlm import HttpNtlmAuth # для работы с логин пароль на кирилице\n",
    "#from requests_negotiate_sspi import HttpNegotiateAuth # негативная аутентификация (без )\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from IPython.display import display  # Для Jupyter\n",
    "from tabulate import tabulate      # Для консоли\n",
    "\n",
    "from urllib.parse import quote_plus # Экранируем специальные символы в пароле\n",
    "\n",
    "\n",
    "#import subprocess # Подключиться к 1С с Windows-аутентификацией. Через powershell -UseDefaultCredentials\n",
    "# Обход блокировки Python Когда нужна нативная аутентификация Windows\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sqlalchemy import create_engine # для создания движка sql\n",
    "from sqlalchemy.sql import text\n",
    "\n",
    "import psycopg2 # обеспечивает взаимодействие между Python-приложениями и базой данных\n",
    "# позволяет выполнять SQL-запросы и получать данные.\n",
    "#print(psycopg2.__version__)  # Должно вывести 2.9.10\n",
    "\n",
    "\n",
    "import hashlib # для хэширования и создания уникальных ключей\n",
    "import uuid # уникальные ключи по UUID\n",
    "\n",
    "import logging\n",
    "\n",
    "#import polars as pl # polars — он потребляет в 3–5 раз меньше памяти чем pandas\n",
    "\n",
    "# настройка логирования\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"sales_1C.log\", encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import urllib3 \n",
    "# Отключаем предупреждение о SSL (только для теста!)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78063de-5a85-48c8-813b-c346d3571fc3",
   "metadata": {},
   "source": [
    "#### Параметры подключения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "458d6182-1f67-4cb8-ad18-4434426907eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем переменные окружения\n",
    "load_dotenv()\n",
    "\n",
    "# Получаем логин и пароль\n",
    "username = os.getenv(\"USERNAME_1C\")\n",
    "password = os.getenv(\"PASSWORD_1C\")\n",
    "\n",
    "if not username or not password:\n",
    "    raise ValueError(\"Логин или пароль не найдены в .env файле!\")\n",
    "\n",
    "\n",
    "# ПАРАМЕТРЫ\n",
    "BUFFER_DAYS = 2       # запас по краям что точно захватить требуемый период\n",
    "# при преобразовании в формат даты с временем для нашей учетной ситсемы, время автоматически будет 00:00:00, ТО все что позже в выборку не попадет\n",
    "# и мы потеряем эти записи\n",
    "CHUNK_DAYS = 7        # оптимальный размер чанка\n",
    "\n",
    "# для загрузки исторических данных\n",
    "DateFrom = '20250101'\n",
    "DateTo = '20250201'\n",
    "\n",
    "# преобразовать в дату, тк timedelta умеет работать только с датой, DateFrom и DateTo в начальном формате это строки\n",
    "expanded_from = datetime.strptime(DateFrom, '%Y%m%d') - timedelta(days=BUFFER_DAYS)\n",
    "expanded_to = datetime.strptime(DateTo, '%Y%m%d') + timedelta(days=BUFFER_DAYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e97c0b9b-db7d-4225-9d31-c6d24980f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция разделяет наш период на более мелкие - по 7 дней (как правило это оптимальный вариант)\n",
    "def date_range_chunks(start_date, end_date, days=7):\n",
    "    start = start_date #datetime.strptime(start_date, '%Y%m%d')\n",
    "    end = end_date #datetime.strptime(end_date, '%Y%m%d')\n",
    "    while start < end:\n",
    "        chunk_end = min(start + timedelta(days=days), end) \n",
    "        yield start.strftime('%Y%m%d'), chunk_end.strftime('%Y%m%d')\n",
    "        start = chunk_end #+timedelta(days=1) \n",
    "# перекрываем чанк на -1 день иначе пограничные даты типа 2025-01-01 23:59:59 выпадают из выборки\n",
    "# в SAP дата отражается дата без времени и все попадает в выборку \n",
    "# тк выборка строится по аналогичной дате до 2025-01-01 00:00:00\n",
    "# далее удалить дубликаты из выборки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "86d5466e-2767-4dcf-baf5-478913b6f80a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:54,711 | INFO | Запрос: 20241230 – 20250106\n",
      "2025-09-30 17:06:54,973 | INFO | ✅ Получено: 24 строк\n",
      "2025-09-30 17:06:54,973 | INFO | ✅ Готово за 0.3 сек\n",
      "2025-09-30 17:06:54,973 | INFO | Запрос: 20250106 – 20250113\n",
      "2025-09-30 17:06:55,205 | INFO | ✅ Получено: 35 строк\n",
      "2025-09-30 17:06:55,207 | INFO | ✅ Готово за 0.2 сек\n",
      "2025-09-30 17:06:55,208 | INFO | Запрос: 20250113 – 20250120\n",
      "2025-09-30 17:06:55,471 | INFO | ✅ Получено: 53 строк\n",
      "2025-09-30 17:06:55,473 | INFO | ✅ Готово за 0.3 сек\n",
      "2025-09-30 17:06:55,473 | INFO | Запрос: 20250120 – 20250127\n",
      "2025-09-30 17:06:55,752 | INFO | ✅ Получено: 51 строк\n",
      "2025-09-30 17:06:55,754 | INFO | ✅ Готово за 0.3 сек\n",
      "2025-09-30 17:06:55,756 | INFO | Запрос: 20250127 – 20250203\n",
      "2025-09-30 17:06:56,221 | INFO | ✅ Получено: 83 строк\n",
      "2025-09-30 17:06:56,223 | INFO | ✅ Готово за 0.5 сек\n",
      "2025-09-30 17:06:56,224 | INFO | ✅ Вся обработка:  1.5 сек\n",
      "2025-09-30 17:06:56,232 | INFO | ✅ DataFrame создан: 246 строк\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "# Кодируем в UTF-8, затем в Base64\n",
    "credentials = f\"{username}:{password}\"\n",
    "credentials_b64 = base64.b64encode(credentials.encode('utf-8')).decode('ascii')\n",
    "# отдельно декодировать логин не получится, т.к. HTTPBasicAuth(login, password) Берёт сырой логин и пароль (например) иванов1:secret \n",
    "# Объединяет: логин:пароль -> Кодирует весь сигнал в байтах (→ тут-то и проблема) -> Кодирует в Base64 -> Передаёт в заголовке -> \n",
    "# Authorization: Basic aXY...base64...\n",
    "# передаем логин:пароль в headers в уже закодированом виде \n",
    " \n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Basic {credentials_b64}',\n",
    "    'User-Agent': 'Python ETL Script'\n",
    "}\n",
    "start_time_all = datetime.now() # время запуска запроса\n",
    "#all_data_rows = [] # добавляем все строки\n",
    "all_data = [] # добавляем все датафреймы по чанкам, это будет список ('list') датафреймов\n",
    "# далее потребуется его преобразовать в привычный pandas -> df_pandas = df_combined.to_pandas()\n",
    "\n",
    "# Основной цикл\n",
    "for date_start, date_end in date_range_chunks(expanded_from, expanded_to, days=CHUNK_DAYS):\n",
    "    start_time = datetime.now() # время запуска запроса\n",
    "    logger.info(f\"Запрос: {date_start} – {date_end}\")\n",
    "    attempt = 0\n",
    "    success = False\n",
    "\n",
    "    while attempt < 3 and not success: # 3 попытки на перезапуск запроса в случае ошибок, обрыва связи и тп\n",
    "            attempt += 1\n",
    "            try:\n",
    "                # url-ссылка указана внутри цикла, т.к. вне цикла будет использована вся ссылка и перебор параметров не пойдет                \n",
    "                url = f'http://example/sale?BeginDate={date_start}&EndDate={date_end}'\n",
    "                headers = {\n",
    "                    'Authorization': f'Basic {credentials_b64}',\n",
    "                    'User-Agent': 'Python ETL Script'\n",
    "                }\n",
    "                \n",
    "                response = requests.get(\n",
    "                    url,\n",
    "                    headers=headers,\n",
    "                    verify=False  # только для теста!\n",
    "                )\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    all_data.extend(data)\n",
    "                    logger.info(f\"✅ Получено: {len(data)} строк\")\n",
    "                    success = True  # ✅ Устанавливаем успех\n",
    "                else:\n",
    "                    print(f\"❌ Ошибка: {response.status_code}, {response.text[:200]}\")\n",
    "            except Exception as e:\n",
    "                    logger.error(f\"❌ Ошибка: {e}\")\n",
    "                    success = False  # ❌ Устанавливаем ошибку\n",
    "            \n",
    "        # повтор цикла если произошла ошибка \n",
    "            if not success and attempt < 3:\n",
    "                logger.info(\"Повтор через 5 сек...\")\n",
    "                time.sleep(5)\n",
    "        \n",
    "    # расчет времени окончания запроса (чанка)\n",
    "    duration = datetime.now() - start_time\n",
    "    logger.info(f\"✅ Готово за {duration.total_seconds():.1f} сек\")\n",
    "                \n",
    "duration_all = datetime.now() - start_time_all\n",
    "logger.info(f\"✅ Вся обработка:  {duration_all.total_seconds():.1f} сек\")\n",
    "\n",
    "# ✅ Теперь сразу обрабатываем\n",
    "if all_data:\n",
    "    df = pd.DataFrame(all_data)\n",
    "        \n",
    "    df['date_doc'] = pd.to_datetime(df['date_doc'], errors='coerce')\n",
    "   \n",
    "        # заполняем пропуски (при первичном анализе выявлены пропуски в полях ниже)\n",
    "    future = pd.Timestamp('2099-12-31 23:59:59') # заглушка\n",
    "    #df['date_transfer_risk'] = df['date_transfer_risk'].fillna(future)\n",
    "    #df['document_date'] = df['document_date'].fillna(future)\n",
    "    \n",
    "    \n",
    "   \n",
    "    logger.info(f\"✅ DataFrame создан: {len(df)} строк\")\n",
    "\n",
    "        \n",
    "else:\n",
    "    logger.info(\"❌ Нет данных для обработки!\")\n",
    "\n",
    "# Теперь можно использовать df дальше\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5237f165-1259-4f03-8989-789f5195d3c0",
   "metadata": {},
   "source": [
    "#### присваиваем hash_id\n",
    "далее удалим дубликаты по hash_id на случай если получили дублирование при перекрытии чанков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "51f86cf6-2597-40b7-a574-2e03e2a8ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hash_id'] = df.astype(str).apply(''.join, axis=1).apply(\n",
    "        lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9b1735ea-cd1a-4af5-b401-8ee751b13f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверяем количество уникальных id через хэш\n",
    "df['hash_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "839d1605-0dc8-407c-ad08-d84d89286b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'количество дубликатов: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nomenclature</th>\n",
       "      <th>buyer</th>\n",
       "      <th>sales_doc</th>\n",
       "      <th>date_doc</th>\n",
       "      <th>region</th>\n",
       "      <th>distribution_channel</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "      <th>revenue_no_nds</th>\n",
       "      <th>tariff</th>\n",
       "      <th>price_in_currency</th>\n",
       "      <th>revenue_in_currency</th>\n",
       "      <th>rate_currency</th>\n",
       "      <th>country</th>\n",
       "      <th>incoterms</th>\n",
       "      <th>hash_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nomenclature, buyer, sales_doc, date_doc, region, distribution_channel, quantity, price, revenue_no_nds, tariff, price_in_currency, revenue_in_currency, rate_currency, country, incoterms, hash_id]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:56,275 | INFO | ✅ Удалены дубликаты по hash_id\n"
     ]
    }
   ],
   "source": [
    "# проверяем количество дубликатов\n",
    "display(f\"количество дубликатов: {df.duplicated().sum()}\")\n",
    "display(df[df.duplicated()])\n",
    "# удаляем дубликаты\n",
    "df = df.drop_duplicates(['hash_id'])\n",
    "# проверяем размер df\n",
    "display(len(df))\n",
    "logger.info(f\"✅ Удалены дубликаты по hash_id\")\n",
    "\n",
    "# дубликаты появляются из-за пограничного времени 00:00:00 в date_doc\n",
    "# в исходном отчете такие дубликаты отсутствуют"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1022c-da5b-4854-a7a7-da5f5b9d7e22",
   "metadata": {},
   "source": [
    "дубликатов нет, количество уникальных hash_id до удаления и после не изменилось"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71019995-a65b-4fcf-98b1-178536d9ef4a",
   "metadata": {},
   "source": [
    "#### Проверяем пропуски, в т.ч. пустые значения ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb0fad00-fee9-476c-b5ce-312cb981248d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Столбцы с пустыми строками: Index(['tariff', 'country', 'incoterms'], dtype='object')\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Проверяем наличие пустых строк, заполнены ''\n",
    "\n",
    "display(f\"Столбцы с пустыми строками: {df.columns[df.eq('').any()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5166b9c6-da1e-4219-af8c-ae60d4559de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем пустые значения на 0 для числовых полей или 'unknown' для текстовых полей\n",
    "df['tariff'] = df['tariff'].replace('', 0)\n",
    "df['country'] = df['country'].replace('', 'unknown')\n",
    "df['incoterms'] = df['incoterms'].replace('', 'unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "901bc144-ec13-4a74-b48e-4e8a9e94b759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 246 entries, 0 to 245\n",
      "Data columns (total 16 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   nomenclature          246 non-null    object        \n",
      " 1   buyer                 246 non-null    object        \n",
      " 2   sales_doc             246 non-null    object        \n",
      " 3   date_doc              246 non-null    datetime64[ns]\n",
      " 4   region                246 non-null    object        \n",
      " 5   distribution_channel  246 non-null    object        \n",
      " 6   quantity              246 non-null    float64       \n",
      " 7   price                 246 non-null    float64       \n",
      " 8   revenue_no_nds        246 non-null    float64       \n",
      " 9   tariff                246 non-null    float64       \n",
      " 10  price_in_currency     246 non-null    float64       \n",
      " 11  revenue_in_currency   246 non-null    float64       \n",
      " 12  rate_currency         246 non-null    float64       \n",
      " 13  country               246 non-null    object        \n",
      " 14  incoterms             246 non-null    object        \n",
      " 15  hash_id               246 non-null    object        \n",
      "dtypes: datetime64[ns](1), float64(7), object(8)\n",
      "memory usage: 30.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bbb5c832-8a9d-4255-82de-3f4cd7c0a9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-01-03 12:00:00')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-02-02 12:00:00')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'количество уникальных sales_doc: 191'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Размер df: 227'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'сумма по полю quantity: 10000.00'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'сумма по полю price: 20000000.00'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'сумма по полю revenue_no_nds: 30000000.00'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Сплошная проверка полноты данных, фильтр на дате отчета\n",
    "display(min(df.query('date_doc > \"2024-12-31 23:59:59\" & date_doc < \"2025-09-02 00:00:00\"')['date_doc']))\n",
    "display(max(df.query('date_doc > \"2024-12-31 23:59:59\" & date_doc < \"2025-09-02 00:00:00\"')['date_doc']))\n",
    "display(f\"количество уникальных sale: {df.query('date_doc > \"2024-12-31 23:59:59\" & date_doc < \"2025-09-02 00:00:00\"')['sale'].nunique()}\")\n",
    "display(f\"Размер df: {len(df.query('date_doc > \"2024-12-31 23:59:59\" & date_doc < \"2025-09-02 00:00:00\"'))}\")\n",
    "display(f\"сумма по полю quantity: {df.query('date_doc > \"2024-12-31 23:59:59\" & date_doc < \"2025-09-02 00:00:00\"')['quantity'].sum()}\")\n",
    "display(f\"сумма по полю price: {df.query('date_doc > \"2024-12-31 23:59:59\" & date_doc < \"2025-09-02 00:00:00\"')['price'].sum()}\")\n",
    "display(f\"сумма по полю revenue_no_nds: {df.query('date_doc > \"2024-12-31 23:59:59\" & date_doc < \"2025-09-02 00:00:00\"')['revenue_no_nds'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c97fa2-77da-46b3-8e0d-9c9571628350",
   "metadata": {},
   "source": [
    "количество уникальных документов 191, размер датафрейма 227, значит в качестве уникального ключа нельзя использовать только номер документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fe344ac8-22f0-4340-b61d-2c4176b122bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ФУНКЦИЙ ПРИСОВЕНИЯ ID\n",
    "\n",
    "# Фуккция присваивает уникальный id как объединение неизменяемых полей (определяется экспертно)\n",
    "def bisness_id(df):\n",
    "    # Базовые бизнес-поля которые идентифицируют запись\n",
    "    id_columns = ['nomenclature', 'buyer', 'sale']\n",
    "    \n",
    "    # Проверка наличия колонок\n",
    "    missing_cols = [col for col in id_columns if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Отсутствуют колонки: {missing_cols}\")\n",
    "    \n",
    "    # Создаем стабильный ключ (даже для дубликатов)\n",
    "    df['bisiness_id'] = df[id_columns].astype(str).apply('_'.join, axis=1)\n",
    "    \n",
    "     # Для дубликатов добавляем порядковый номер\n",
    "    df['duplicate_index'] = df.groupby(id_columns).cumcount()\n",
    "\n",
    "    # Создаем уникальный id\n",
    "    df['unique_id'] = df['bisiness_id'] + '_' + df['duplicate_index'].astype(str)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "25a8927f-2c55-443b-a001-fcfb7012c1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:56,384 | INFO | Уникальных ID df: 246\n"
     ]
    }
   ],
   "source": [
    "sale = bisness_id(df) \n",
    "    # Проверяем стабильность\n",
    "logger.info(f\"Уникальных ID df: {sale['unique_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c3242666-5bf1-4694-b102-e64f4e37453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим время загрузки\n",
    "sale['load_time'] = pd.Timestamp.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c3047688-375b-4c08-b24b-2edf443bf09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 246 entries, 0 to 245\n",
      "Data columns (total 20 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   nomenclature          246 non-null    object        \n",
      " 1   buyer                 246 non-null    object        \n",
      " 2   sales_doc             246 non-null    object        \n",
      " 3   date_doc              246 non-null    datetime64[ns]\n",
      " 4   region                246 non-null    object        \n",
      " 5   distribution_channel  246 non-null    object        \n",
      " 6   quantity              246 non-null    float64       \n",
      " 7   price                 246 non-null    float64       \n",
      " 8   revenue_no_nds        246 non-null    float64       \n",
      " 9   tariff                246 non-null    float64       \n",
      " 10  price_in_currency     246 non-null    float64       \n",
      " 11  revenue_in_currency   246 non-null    float64       \n",
      " 12  rate_currency         246 non-null    float64       \n",
      " 13  country               246 non-null    object        \n",
      " 14  incoterms             246 non-null    object        \n",
      " 15  hash_id               246 non-null    object        \n",
      " 16  bisiness_id           246 non-null    object        \n",
      " 17  duplicate_index       246 non-null    int64         \n",
      " 18  unique_id             246 non-null    object        \n",
      " 19  load_time             246 non-null    datetime64[us]\n",
      "dtypes: datetime64[ns](1), datetime64[us](1), float64(7), int64(1), object(10)\n",
      "memory usage: 38.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sale.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb467a-1f2a-4e77-b4e1-cfd48559f3cc",
   "metadata": {},
   "source": [
    "#### ПРОВЕРКИ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bc6c3-3988-40a5-86c4-58f00a4e575f",
   "metadata": {},
   "source": [
    "####   Подключение к БД PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d550b1ab-412a-4075-8a3b-9f5ea08866df",
   "metadata": {},
   "source": [
    "#### Загрузка исторических данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "25853a08-1a3a-4f3b-aa2b-564d4f964da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тестовый датафрейм: 246 строк, 20 столбцов\n",
      "✅ Данные загружены: 246 строк в таблицу dm.test_sale\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Читаем параметры подключения\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_port = os.getenv(\"DB_PORT\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "\n",
    "\n",
    "print(f\"Тестовый датафрейм: {sale.shape[0]} строк, {sale.shape[1]} столбцов\")\n",
    "\n",
    "# Экранируем пароль\n",
    "encoded_password = quote_plus(db_password)\n",
    "\n",
    "db_url = f\"postgresql+psycopg2://{db_user}:{encoded_password}@{db_host}:{db_port}/{db_name}\"\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Создаём движок для подключения по ссылке engine\n",
    "engine = create_engine(db_url, pool_pre_ping=True) \n",
    "# используем дополнительно pool_pre_ping=True\n",
    "# если соединение «умрёт» между запросами — SQLAlchemy автоматически его заменяет, и вы никогда не увидите ошибку «битого соединения»\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Загружаем в PostgreSQL\n",
    "    sale.to_sql(\n",
    "        name='test_sale',           # имя таблицы\n",
    "        schema='dm',                    # схема\n",
    "        con=engine,\n",
    "        if_exists='replace',            # перезаписать, если есть        \n",
    "        index=False,                    # не сохранять индекс\n",
    "        method='multi',                 # немного быстрее при вставке\n",
    "        chunksize=10                    # пачками — меньше нагрузки\n",
    "    )\n",
    "    print(f\"✅ Данные загружены: {len(sale)} строк в таблицу dm.test_sale\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Ошибка при загрузке в БД: {e}\")\n",
    "\n",
    "finally:\n",
    "    engine.dispose()  # закрываем соединение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309f007c-d539-46cc-a08d-b4903caf964d",
   "metadata": {},
   "source": [
    "# Прочитаем обратно\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207c425-b12f-4cf0-b0c5-b16471635b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sale = pd.read_sql(\"SELECT * FROM dm.test_sale LIMIT 2\", engine)\n",
    "display(df_test_sale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ec393-a9f0-437c-8a2f-1f8abaa8b736",
   "metadata": {},
   "source": [
    "#### Добавляем в таблицу в БД ADD CONSTRAINT\n",
    "    для работы с ON CONFLICT (unique_id) DO UPDATE\n",
    "    после загрузки историчеких данных, предполагается что таблица уже есть в БД\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c9ec349d-d5b2-42e7-b288-c5a1177fd4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:56,542 | INFO | ✅ Соединение с БД очищено\n",
      "2025-09-30 17:06:56,543 | INFO | ✅ Новое подключение к БД создано\n",
      "2025-09-30 17:06:56,578 | INFO | ✅ Ограничение uk_unique_id добавлено\n"
     ]
    }
   ],
   "source": [
    "# 1. Очистка предыдущего состояния ---\n",
    "try:\n",
    "    with engine.begin() as conn:\n",
    "        pass  # триггер на rollback, если была ошибка\n",
    "except Exception as e:\n",
    "    logging.warning(f\"Транзакция была в состоянии ошибки, выполнен rollback: {e}\")\n",
    "\n",
    "engine.dispose()\n",
    "logging.info(\"✅ Соединение с БД очищено\")\n",
    "\n",
    "# Пересоздаём engine\n",
    "engine = create_engine(db_url)\n",
    "logging.info(\"✅ Новое подключение к БД создано\")\n",
    "\n",
    "\n",
    "# 2. Проверка и создание UNIQUE CONSTRAINT ---\n",
    "constraint_name = 'uk_unique_id' # долюно быть уникальное имя в рамках одной схемы для каждой таблицы внутри схемы\n",
    "table_schema = 'dm'\n",
    "table_name = 'test_sale'\n",
    "\n",
    "check_sql = f\"\"\"\n",
    "SELECT 1 FROM information_schema.table_constraints \n",
    "WHERE table_schema = '{table_schema}' \n",
    "  AND table_name = '{table_name}'\n",
    "  AND constraint_name = '{constraint_name}';\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    result = pd.read_sql(check_sql, engine)\n",
    "    if len(result) == 0:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(f\"\"\"\n",
    "                ALTER TABLE {table_schema}.{table_name}\n",
    "                ADD CONSTRAINT {constraint_name} UNIQUE (unique_id)\n",
    "            \"\"\"))\n",
    "        logging.info(f\"✅ Ограничение {constraint_name} добавлено\")\n",
    "    else:\n",
    "        logging.info(f\"🟢 Ограничение {constraint_name} уже существует — пропускаем\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"❌ Ошибка при работе с ограничением: {e}\")\n",
    "    raise  # или continue, в зависимости от стратегии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30435752-19a4-4b47-ae7f-93f07b16c389",
   "metadata": {},
   "source": [
    "#### Подготовка нового датафрейма исторические + новые данные \n",
    "тестируем инкрементальную загрузку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b3a67ccb-d263-4fb9-ac53-b5e05e4c9da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ПАРАМЕТРЫ\n",
    "BUFFER_DAYS = 2       # запас по краям\n",
    "CHUNK_DAYS = 7        # оптимальный размер чанка\n",
    "\n",
    "# для загрузки исторических данных ПЕРИОД 1\n",
    "DateFrom_new = '20250115'\n",
    "DateTo_new = '20250301'\n",
    "\n",
    "# преобразовать в дату, тк timedelta умеет работать только с датой, DateFrom и DateTo в начальном формате это строки\n",
    "expanded_from_new = datetime.strptime(DateFrom_new, '%Y%m%d') - timedelta(days=BUFFER_DAYS)\n",
    "expanded_to_new = datetime.strptime(DateTo_new, '%Y%m%d') + timedelta(days=BUFFER_DAYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6df807df-aff0-43c6-86d5-2fe5bda62105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:56,592 | INFO | Запрос: 20250113 – 20250120\n",
      "2025-09-30 17:06:56,894 | INFO | ✅ Получено: 53 строк\n",
      "2025-09-30 17:06:56,896 | INFO | ✅ Готово за 0.3 сек\n",
      "2025-09-30 17:06:56,896 | INFO | Запрос: 20250120 – 20250127\n",
      "2025-09-30 17:06:57,211 | INFO | ✅ Получено: 51 строк\n",
      "2025-09-30 17:06:57,211 | INFO | ✅ Готово за 0.3 сек\n",
      "2025-09-30 17:06:57,211 | INFO | Запрос: 20250127 – 20250203\n",
      "2025-09-30 17:06:57,644 | INFO | ✅ Получено: 83 строк\n",
      "2025-09-30 17:06:57,645 | INFO | ✅ Готово за 0.4 сек\n",
      "2025-09-30 17:06:57,645 | INFO | Запрос: 20250203 – 20250210\n",
      "2025-09-30 17:06:57,944 | INFO | ✅ Получено: 51 строк\n",
      "2025-09-30 17:06:57,944 | INFO | ✅ Готово за 0.3 сек\n",
      "2025-09-30 17:06:57,944 | INFO | Запрос: 20250210 – 20250217\n",
      "2025-09-30 17:06:58,311 | INFO | ✅ Получено: 78 строк\n",
      "2025-09-30 17:06:58,312 | INFO | ✅ Готово за 0.4 сек\n",
      "2025-09-30 17:06:58,312 | INFO | Запрос: 20250217 – 20250224\n",
      "2025-09-30 17:06:58,635 | INFO | ✅ Получено: 54 строк\n",
      "2025-09-30 17:06:58,635 | INFO | ✅ Готово за 0.3 сек\n",
      "2025-09-30 17:06:58,635 | INFO | Запрос: 20250224 – 20250303\n",
      "2025-09-30 17:06:59,024 | INFO | ✅ Получено: 72 строк\n",
      "2025-09-30 17:06:59,025 | INFO | ✅ Готово за 0.4 сек\n",
      "2025-09-30 17:06:59,026 | INFO | ✅ Вся обработка:  2.4 сек\n",
      "2025-09-30 17:06:59,030 | INFO | ✅ DataFrame создан: 442 строк\n"
     ]
    }
   ],
   "source": [
    "# Подготовка нового датафрейма исторические + новые данные \n",
    "\n",
    "import base64\n",
    "# Кодируем в UTF-8, затем в Base64\n",
    "credentials = f\"{username}:{password}\"\n",
    "credentials_b64 = base64.b64encode(credentials.encode('utf-8')).decode('ascii')\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Basic {credentials_b64}',\n",
    "    'User-Agent': 'Python ETL Script'\n",
    "}\n",
    "start_time_all = datetime.now() # время запуска запроса\n",
    "\n",
    "all_data_new = [] \n",
    "\n",
    "# Основной цикл\n",
    "for date_start, date_end in date_range_chunks(expanded_from_new, expanded_to_new, days=CHUNK_DAYS):\n",
    "    start_time = datetime.now() # время запуска запроса\n",
    "    logger.info(f\"Запрос: {date_start} – {date_end}\")\n",
    "    attempt = 0\n",
    "    success = False\n",
    "\n",
    "    while attempt < 3 and not success: # 3 попытки на перезапуск запроса в случае ошибок, обрыва связи и тп\n",
    "            attempt += 1\n",
    "            try:\n",
    "                # url-ссылка указана внутри цикла, т.к. вне цикла будет использована вся ссылка и перебор параметров не пойдет\n",
    "                url = f'http://example/sale?BeginDate={date_start}&EndDate={date_end}'\n",
    "                headers = {\n",
    "                    'Authorization': f'Basic {credentials_b64}',\n",
    "                    'User-Agent': 'Python ETL Script'\n",
    "                }\n",
    "                \n",
    "                response = requests.get(\n",
    "                    url,\n",
    "                    headers=headers,\n",
    "                    verify=False  # только для теста!\n",
    "                )\n",
    "                if response.status_code == 200:\n",
    "                    data_new = response.json()\n",
    "                    all_data_new.extend(data_new)\n",
    "                    logger.info(f\"✅ Получено: {len(data_new)} строк\")\n",
    "                    success = True  # ✅ Устанавливаем успех\n",
    "                else:\n",
    "                    print(f\"❌ Ошибка: {response.status_code}, {response.text[:200]}\")\n",
    "            except Exception as e:\n",
    "                    logger.error(f\"❌ Ошибка: {e}\")\n",
    "                    success = False  # ❌ Устанавливаем ошибку\n",
    "            \n",
    "        # повтор цикла если произошла ошибка \n",
    "            if not success and attempt < 3:\n",
    "                logger.info(\"Повтор через 5 сек...\")\n",
    "                time.sleep(5)\n",
    "        \n",
    "    # расчет времени окончания запроса (чанка)\n",
    "    duration = datetime.now() - start_time\n",
    "    logger.info(f\"✅ Готово за {duration.total_seconds():.1f} сек\")\n",
    "                \n",
    "duration_all = datetime.now() - start_time_all\n",
    "logger.info(f\"✅ Вся обработка:  {duration_all.total_seconds():.1f} сек\")\n",
    "\n",
    "# ✅ Теперь сразу обрабатываем\n",
    "if all_data_new:\n",
    "    df_new = pd.DataFrame(all_data_new)\n",
    "        \n",
    "    df_new['date_doc'] = pd.to_datetime(df_new['date_doc'], errors='coerce')\n",
    "    \n",
    "        # заполняем пропуски (при первичном анализе выявлены пропуски в полях ниже)\n",
    "    future = pd.Timestamp('2099-12-31 23:59:59') # заглушка\n",
    "    df_new['date_transfer_risk'] = df_new['date_transfer_risk'].fillna(future)\n",
    "    #df_new['document_date'] = df_new['document_date'].fillna(future)\n",
    "\n",
    "    logger.info(f\"✅ DataFrame создан: {len(df_new)} строк\")\n",
    "\n",
    "        \n",
    "else:\n",
    "    logger.info(\"❌ Нет данных для обработки!\")\n",
    "\n",
    "# Теперь можно использовать df дальше\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fefe1d3-9e00-4289-893e-281e0dc8f655",
   "metadata": {},
   "source": [
    "#### присваиваем hash_id\n",
    "далее удалим дубликаты по hash_id на случай если получили дублирование при перекрытии чанков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "95c68b16-b77a-4ef8-9d10-b0d9c7d1c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['hash_id'] = df_new.astype(str).apply(''.join, axis=1).apply(\n",
    "        lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bf755fe8-cca1-4c4d-b8fb-bfddeba93268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверяем количество уникальных id через хэш\n",
    "df_new['hash_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cadff8bd-2df0-4d20-bc26-6e10b5274845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'количество дубликатов: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nomenclature</th>\n",
       "      <th>buyer</th>\n",
       "      <th>sales_doc</th>\n",
       "      <th>date_doc</th>\n",
       "      <th>region</th>\n",
       "      <th>distribution_channel</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "      <th>revenue_no_nds</th>\n",
       "      <th>tariff</th>\n",
       "      <th>price_in_currency</th>\n",
       "      <th>revenue_in_currency</th>\n",
       "      <th>rate_currency</th>\n",
       "      <th>country</th>\n",
       "      <th>incoterms</th>\n",
       "      <th>hash_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nomenclature, buyer, sales_doc, date_doc, region, distribution_channel, quantity, price, revenue_no_nds, tariff, price_in_currency, revenue_in_currency, rate_currency, country, incoterms, hash_id]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:59,094 | INFO | ✅ Удалены дубликаты по hash_id\n"
     ]
    }
   ],
   "source": [
    "# проверяем количество дубликатов\n",
    "display(f\"количество дубликатов: {df_new.duplicated().sum()}\")\n",
    "display(df_new[df_new.duplicated()])\n",
    "# удаляем дубликаты\n",
    "df_new = df_new.drop_duplicates(['hash_id'])\n",
    "# проверяем размер df\n",
    "display(len(df_new))\n",
    "logger.info(f\"✅ Удалены дубликаты по hash_id\")\n",
    "\n",
    "# дубликаты появляются из-за пограничного времени 00:00:00 в date_doc\n",
    "# в исходном отчете такие дубликаты отсутствуют"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e6a26-2c79-4a03-9d02-68ec8073c70f",
   "metadata": {},
   "source": [
    "#### Проверяем пропуски, в т.ч. пустые значения ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9700277d-9a5a-4828-83e7-2c680855ba69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Столбцы с пустыми строками: Index(['tariff', 'country', 'incoterms'], dtype='object')\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Проверяем наличие пустых строк, заполнены ''\n",
    "\n",
    "display(f\"Столбцы с пустыми строками: {df_new.columns[df_new.eq('').any()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ff0ac5b5-d145-4e94-8934-2eeb3b0006a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем пустные значения, аналогично как с обработкой исторических данных\n",
    "df_new['tariff'] = df_new['tariff'].replace('', 0)\n",
    "df_new['country'] = df_new['country'].replace('', 'unknown')\n",
    "df_new['incoterms'] = df_new['incoterms'].replace('', 'unknown')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f43549ad-73e9-4878-953e-33d65aa54522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 442 entries, 0 to 441\n",
      "Data columns (total 16 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   nomenclature          442 non-null    object        \n",
      " 1   buyer                 442 non-null    object        \n",
      " 2   sales_doc             442 non-null    object        \n",
      " 3   date_doc              442 non-null    datetime64[ns]\n",
      " 4   region                442 non-null    object        \n",
      " 5   distribution_channel  442 non-null    object        \n",
      " 6   quantity              442 non-null    float64       \n",
      " 7   price                 442 non-null    float64       \n",
      " 8   revenue_no_nds        442 non-null    float64       \n",
      " 9   tariff                442 non-null    float64       \n",
      " 10  price_in_currency     442 non-null    float64       \n",
      " 11  revenue_in_currency   442 non-null    float64       \n",
      " 12  rate_currency         442 non-null    float64       \n",
      " 13  country               442 non-null    object        \n",
      " 14  incoterms             442 non-null    object        \n",
      " 15  hash_id               442 non-null    object        \n",
      "dtypes: datetime64[ns](1), float64(7), object(8)\n",
      "memory usage: 55.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_new.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f88970d8-9f3d-4d51-be43-a62efcd7e0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:59,134 | INFO | ✅ Удалены дубликаты: 0\n",
      "2025-09-30 17:06:59,136 | INFO | В таблице записей: 442\n"
     ]
    }
   ],
   "source": [
    "# удаляем дубликаты\n",
    "df_new = df_new.drop_duplicates()\n",
    "logger.info(f\"✅ Удалены дубликаты: {df_new.duplicated().sum()}\")\n",
    "logger.info(f\"В таблице записей: {len(df_new)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8b9ab060-51aa-48ae-8a20-13caf4b7686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:59,148 | INFO | Уникальных unique_id: 442\n"
     ]
    }
   ],
   "source": [
    "df_new = bisness_id(df_new) #, salt='one_s_analyse_revenue') если потребуется дополнительно идентификатор\n",
    "    # Проверяем стабильность\n",
    "logger.info(f\"Уникальных unique_id: {df_new['unique_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "940f2e00-d904-4097-bb33-4fbcd410f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим время загрузки\n",
    "df_new['load_time'] = pd.Timestamp.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8a032-ad48-4048-94ed-ce5eb9a5b913",
   "metadata": {},
   "source": [
    "#### Инкрементальная загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "06e9e1cf-66cf-4c7b-a09f-f56e5c2febae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:59,160 | INFO | ✅ Подключение к БД готово. Проверяем таблицу...\n"
     ]
    }
   ],
   "source": [
    "# Подключение к БД\n",
    "# Читаем пароль\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_port = os.getenv(\"DB_PORT\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "\n",
    "# Экранируем пароль\n",
    "encoded_password = quote_plus(db_password)\n",
    "\n",
    "# создаем ссылку на подключение\n",
    "db_url = f\"postgresql+psycopg2://{db_user}:{encoded_password}@{db_host}:{db_port}/{db_name}\"\n",
    "\n",
    "# Создаём движок для подключения по ссылке engine\n",
    "engine = create_engine(db_url, pool_pre_ping=True) \n",
    "# используем дополнительно pool_pre_ping=True\n",
    "# если соединение «умрёт» между запросами — SQLAlchemy автоматически его заменяет, и вы никогда не увидите ошибку «битого соединения»\n",
    "\n",
    "\n",
    "logger.info(\"✅ Подключение к БД готово. Проверяем таблицу...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "377d0ceb-f674-474b-88e5-0dcf26a6db99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:59,193 | INFO | 📊 В таблице dm.test_sale: 246 строк\n"
     ]
    }
   ],
   "source": [
    "# Проверка наличия таблицы и количества записей\n",
    "query_count = \"\"\"\n",
    "SELECT COUNT(*) AS total_count \n",
    "FROM dm.test_sale;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    result = pd.read_sql(query_count, engine)\n",
    "    logger.info(f\"📊 В таблице dm.test_sale: {result.iloc[0]['total_count']} строк\")\n",
    "except Exception as e:\n",
    "    logger.info(f\"❌ Ошибка при запросе к таблице: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e331a880-e5f0-4a44-a75c-73c6cf3daef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:59,205 | INFO | ✅ Успешно загружено 246 уникальных ID из БД\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры ID из БД: ['.....']\n"
     ]
    }
   ],
   "source": [
    "# Забираем все id из БД\n",
    "query_ids = \"\"\"\n",
    "SELECT DISTINCT unique_id\n",
    "FROM dm.test_sale\n",
    "WHERE unique_id IS NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    existing_df = pd.read_sql(query_ids, engine)\n",
    "    existing_ids = set(existing_df['unique_id'])\n",
    "    logger.info(f\"✅ Успешно загружено {len(existing_ids)} уникальных ID из БД\")\n",
    "    # Покажем несколько первых\n",
    "    print(\"Примеры ID из БД:\", list(existing_ids)[:5])\n",
    "except Exception as e:\n",
    "    logger.info(f\"❌ Не удалось загрузить unique_id: {e}\")\n",
    "    existing_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cb394ae5-9271-49bc-8df0-b5f4ab6191c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:59,212 | INFO | ✅ В df_new найден столбец unique_id. Всего строк: 442\n",
      "2025-09-30 17:06:59,212 | INFO | 🔁 Уже есть в БД: 187\n",
      "2025-09-30 17:06:59,213 | INFO | 🆕 Новых ID: 255\n",
      "2025-09-30 17:06:59,215 | INFO | ✅ Отфильтровано для загрузки: 255 строк\n"
     ]
    }
   ],
   "source": [
    "# Проверим, есть ли unique_id в df\n",
    "if 'unique_id' not in df_new.columns:\n",
    "    logger.info(\"❌ Ошибка: в df нет столбца unique_id\")\n",
    "else:\n",
    "    logger.info(f\"✅ В df_new найден столбец unique_id. Всего строк: {len(df_new)}\")\n",
    "\n",
    "    # Преобразуем в set для быстрого сравнения\n",
    "    new_ids = set(df_new['unique_id'])\n",
    "\n",
    "    # Находим, сколько уже есть в БД\n",
    "    already_exists = new_ids & existing_ids \n",
    "# Оператор & между двумя set в Python означает пересечение: возвращает новый set, содержащий только те элементы, которые присутствуют в обоих множествах.    \n",
    "     \n",
    "    brand_new = new_ids - existing_ids\n",
    "# возвращает новый set состящий из элементо отсутствующих в новом set\n",
    "\n",
    "    logger.info(f\"🔁 Уже есть в БД: {len(already_exists)}\")\n",
    "    logger.info(f\"🆕 Новых ID: {len(brand_new)}\")\n",
    "\n",
    "    # Фильтруем df — только новые строки\n",
    "    df_new_id = df_new[df_new['unique_id'].isin(brand_new)].copy()\n",
    "\n",
    "    logger.info(f\"✅ Отфильтровано для загрузки: {len(df_new_id)} строк\")\n",
    "    \n",
    "    # Проверим, нет ли дублей\n",
    "    if df_new_id['unique_id'].duplicated().any():\n",
    "        logger.info(\"⚠️ Найдены дубли по unique_id в новых данных!\")\n",
    "        df_new_id.drop_duplicates(subset=['unique_id'], keep='first', inplace=True)\n",
    "        logger.info(f\"✅ После удаления дублей: {len(df_new_id)} строк\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c2add4f9-f5fc-48f5-86d9-42d1ac61ba37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:59,223 | INFO | ✅ Успешно загружено 246 уникальных hash_id из БД\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры ID из БД: ['.....']\n"
     ]
    }
   ],
   "source": [
    "# Забираем все hash_id из БД\n",
    "query_hash_id = \"\"\"\n",
    "SELECT DISTINCT hash_id\n",
    "FROM dm.test_sale\n",
    "WHERE unique_id IS NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    existing_df_hash_id = pd.read_sql(query_hash_id, engine)\n",
    "    existing_df_hash_id = set(existing_df_hash_id['hash_id'])\n",
    "    logger.info(f\"✅ Успешно загружено {len(existing_df_hash_id)} уникальных hash_id из БД\")\n",
    "    # Покажем несколько первых\n",
    "    print(\"Примеры ID из БД:\", list(existing_df_hash_id)[:5])\n",
    "except Exception as e:\n",
    "    logger.info(f\"❌ Не удалось загрузить unique_id: {e}\")\n",
    "    existing_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1001b6ed-6205-46dd-8710-22154801b463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:59,234 | INFO | ✅ В df_new найден столбец unique_id. Всего строк: 442\n",
      "2025-09-30 17:06:59,234 | INFO | 🔁 Уже есть в БД: 187\n",
      "2025-09-30 17:06:59,235 | INFO | 🆕 Новых ID: 255\n",
      "2025-09-30 17:06:59,237 | INFO | ✅ Отфильтровано для загрузки: 255 строк\n"
     ]
    }
   ],
   "source": [
    "# Проверим, есть ли hash_id в df\n",
    "if 'hash_id' not in df_new.columns:\n",
    "    logger.info(\"❌ Ошибка: в df нет столбца hash_id\")\n",
    "else:\n",
    "    logger.info(f\"✅ В df_new найден столбец unique_id. Всего строк: {len(df_new)}\")\n",
    "\n",
    "    # Преобразуем в set для быстрого сравнения\n",
    "    new_hash_id = set(df_new['hash_id'])\n",
    "\n",
    "    # Находим, сколько уже есть в БД\n",
    "    already_exists_hash_id = new_hash_id & existing_df_hash_id \n",
    "# Оператор & между двумя set в Python означает пересечение: возвращает новый set, содержащий только те элементы, которые присутствуют в обоих множествах.    \n",
    "     \n",
    "    brand_new_hash_id = new_hash_id - existing_df_hash_id\n",
    "# возвращает новый set состящий из элементо отсутствующих в новом set\n",
    "\n",
    "    logger.info(f\"🔁 Уже есть в БД: {len(already_exists_hash_id)}\")\n",
    "    logger.info(f\"🆕 Новых ID: {len(brand_new_hash_id)}\")\n",
    "\n",
    "    # Фильтруем df — только новые строки\n",
    "    df_new_hash_id = df_new[df_new['hash_id'].isin(brand_new_hash_id)].copy()\n",
    "\n",
    "    logger.info(f\"✅ Отфильтровано для загрузки: {len(df_new_hash_id)} строк\")\n",
    "    \n",
    "    # Проверим, нет ли дублей\n",
    "    if df_new_hash_id['hash_id'].duplicated().any():\n",
    "        logger.info(\"⚠️ Найдены дубли по hash_id в новых данных!\")\n",
    "        df_new_hash_id.drop_duplicates(subset=['hash_id'], keep='first', inplace=True)\n",
    "        logger.info(f\"✅ После удаления дублей: {len(df_new_hash_id)} строк\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2cd559-8b8a-48e2-98d9-7e59539238ca",
   "metadata": {},
   "source": [
    "#### UPSERT через временную таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0e2fd910-815d-40e0-81b8-ea6b6e07d93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:59,378 | INFO | ✅ Данные во временной таблице\n"
     ]
    }
   ],
   "source": [
    "# Шаг 1: Создаём временную таблицу\n",
    "df_new.to_sql(\n",
    "    name='temp_sale',\n",
    "    con=engine,\n",
    "    schema='dm', # песочница\n",
    "    if_exists='replace',  # пересоздаём каждый раз\n",
    "    index=False,\n",
    "    method='multi',\n",
    "    chunksize=1000\n",
    ")\n",
    "\n",
    "logger.info(\"✅ Данные во временной таблице\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d8ff2861-a5a0-4e9b-aa37-f61a782648fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:59,394 | INFO | ✅ UPSERT выполнен: 255 строк добавлено/обновлено\n"
     ]
    }
   ],
   "source": [
    "# Шаг 2: Выполняем UPSERT\n",
    "# задаем переменную обновления строк\n",
    "upsert_sql = \"\"\"\n",
    "    INSERT INTO dm.test_sale AS target\n",
    "    SELECT \n",
    "        nomenclature,\n",
    "        buyer,\n",
    "        sale,\n",
    "        date_doc,\n",
    "        region,\n",
    "        distribution_channel,\n",
    "        quantity, \n",
    "        price,\n",
    "        revenue_no_nds,\n",
    "        tariff,\n",
    "        price_in_currency,\n",
    "        revenue_in_currency,\n",
    "        rate_currency,\n",
    "        country,\n",
    "        incoterms,\n",
    "        hash_id,\n",
    "        bisiness_id,\n",
    "        duplicate_index,\n",
    "        unique_id,\n",
    "        load_time\n",
    "    FROM dm.temp_sale\n",
    "    ON CONFLICT (unique_id)\n",
    "    DO UPDATE SET\n",
    "        nomenclature = EXCLUDED.nomenclature,\n",
    "        buyer = EXCLUDED.buyer,\n",
    "        sale = EXCLUDED.sale,\n",
    "        date_doc = EXCLUDED.date_doc,\n",
    "        region = EXCLUDED.region,\n",
    "        distribution_channel = EXCLUDED.distribution_channel,\n",
    "        quantity = EXCLUDED.quantity,\n",
    "        price = EXCLUDED.price,\n",
    "        revenue_no_nds = EXCLUDED.revenue_no_nds,\n",
    "        tariff = EXCLUDED.tariff,\n",
    "        price_in_currency = EXCLUDED.price_in_currency,\n",
    "        revenue_in_currency = EXCLUDED.revenue_in_currency,\n",
    "        rate_currency = EXCLUDED.rate_currency,\n",
    "        country = EXCLUDED.country,\n",
    "        incoterms = EXCLUDED.incoterms,\n",
    "        hash_id = EXCLUDED.hash_id,\n",
    "        bisiness_id = EXCLUDED.bisiness_id,\n",
    "        duplicate_index = EXCLUDED.duplicate_index,\n",
    "        unique_id = EXCLUDED.unique_id,\n",
    "        load_time = EXCLUDED.load_time                \n",
    "    WHERE target.hash_id IS DISTINCT FROM EXCLUDED.hash_id;\n",
    "\"\"\"\n",
    "# Обновляет только если хэш (hash_id)  изменится\n",
    "\n",
    "\n",
    "# Выполняем в транзакции\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(upsert_sql))\n",
    "\n",
    "logger.info(f\"✅ UPSERT выполнен: {len(df_new_id)} строк добавлено/обновлено\")\n",
    "\n",
    "# Шаг 3: Удаляем временную таблицу\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"DROP TABLE IF EXISTS dm.temp_sale\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a4570a85-4b7a-4465-be5c-4dbacaec5147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:59,407 | INFO | 📊 Теперь в таблице: 501 строк\n"
     ]
    }
   ],
   "source": [
    "# Проверка сколько стало строк в БД после добавление новых строк\n",
    "query_count_after = \"\"\"\n",
    "SELECT COUNT(*) AS total_count \n",
    "FROM dm.test_sale;\n",
    "\"\"\"\n",
    "\n",
    "result_after = pd.read_sql(query_count_after, engine)\n",
    "logger.info(f\"📊 Теперь в таблице: {result_after.iloc[0]['total_count']} строк\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dda72f-ede8-409c-8eb5-faa6b98fae33",
   "metadata": {},
   "source": [
    "после запись новых строк получили 246 из таблицы с историческими данными + 255 стр из новой таблицы = 501, все правильно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e9f1f434-004c-42ca-b6d7-9d8c766c2b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sum_quantity     sum_price  sum_revenue_no_nds\n",
      "0      25000.00   50000000.00         70000000.00"
     ]
    }
   ],
   "source": [
    "# Проверка суммы по числовым полям  в БД\n",
    "query_count_after = \"\"\"\n",
    "SELECT \n",
    "    sum(quantity) as sum_quantity,\n",
    "    sum(price)  as sum_price,\n",
    "    sum(revenue_no_nds)  as sum_revenue_no_nds\n",
    "FROM dm.test_sale;\n",
    "\"\"\"\n",
    "\n",
    "result_after = pd.read_sql(query_count_after, engine)\n",
    "print(result_after)\n",
    "#logger.info(f\"📊 Теперь в таблице: {result_after.iloc[0]['total_count']} строк\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71e1674-07c4-4896-9363-d7a3766bc7c1",
   "metadata": {},
   "source": [
    "#### Тест изменения данных в исходных данных\n",
    "за основу берем 2й период, числовые поля х2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4e31a810-9550-4c65-b746-cf97a630e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:06:59,427 | INFO | Запрос: 20250113 – 20250120\n",
      "2025-09-30 17:06:59,740 | INFO | ✅ Получено: 53 строк\n",
      "2025-09-30 17:06:59,741 | INFO | ✅ Готово за 0.3 сек\n",
      "2025-09-30 17:06:59,742 | INFO | Запрос: 20250120 – 20250127\n",
      "2025-09-30 17:07:00,025 | INFO | ✅ Получено: 51 строк\n",
      "2025-09-30 17:07:00,026 | INFO | ✅ Готово за 0.3 сек\n",
      "2025-09-30 17:07:00,027 | INFO | Запрос: 20250127 – 20250203\n",
      "2025-09-30 17:07:00,482 | INFO | ✅ Получено: 83 строк\n",
      "2025-09-30 17:07:00,483 | INFO | ✅ Готово за 0.5 сек\n",
      "2025-09-30 17:07:00,484 | INFO | Запрос: 20250203 – 20250210\n",
      "2025-09-30 17:07:00,790 | INFO | ✅ Получено: 51 строк\n",
      "2025-09-30 17:07:00,791 | INFO | ✅ Готово за 0.3 сек\n",
      "2025-09-30 17:07:00,792 | INFO | Запрос: 20250210 – 20250217\n",
      "2025-09-30 17:07:01,165 | INFO | ✅ Получено: 78 строк\n",
      "2025-09-30 17:07:01,165 | INFO | ✅ Готово за 0.4 сек\n",
      "2025-09-30 17:07:01,166 | INFO | Запрос: 20250217 – 20250224\n",
      "2025-09-30 17:07:01,455 | INFO | ✅ Получено: 54 строк\n",
      "2025-09-30 17:07:01,456 | INFO | ✅ Готово за 0.3 сек\n",
      "2025-09-30 17:07:01,457 | INFO | Запрос: 20250224 – 20250303\n",
      "2025-09-30 17:07:01,922 | INFO | ✅ Получено: 72 строк\n",
      "2025-09-30 17:07:01,924 | INFO | ✅ Готово за 0.5 сек\n",
      "2025-09-30 17:07:01,924 | INFO | ✅ Вся обработка:  2.5 сек\n",
      "2025-09-30 17:07:01,928 | INFO | ✅ DataFrame создан: 442 строк\n"
     ]
    }
   ],
   "source": [
    "# Подготовка нового датафрейма исторические + новые данные \n",
    "\n",
    "import base64\n",
    "# Кодируем в UTF-8, затем в Base64\n",
    "credentials = f\"{username}:{password}\"\n",
    "credentials_b64 = base64.b64encode(credentials.encode('utf-8')).decode('ascii')\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Basic {credentials_b64}',\n",
    "    'User-Agent': 'Python ETL Script'\n",
    "}\n",
    "start_time_all = datetime.now() # время запуска запроса\n",
    "\n",
    "all_data_new_2 = [] \n",
    "\n",
    "# Основной цикл\n",
    "for date_start, date_end in date_range_chunks(expanded_from_new, expanded_to_new, days=CHUNK_DAYS):\n",
    "    start_time = datetime.now() # время запуска запроса\n",
    "    logger.info(f\"Запрос: {date_start} – {date_end}\")\n",
    "    attempt = 0\n",
    "    success = False\n",
    "\n",
    "    while attempt < 3 and not success: # 3 попытки на перезапуск запроса в случае ошибок, обрыва связи и тп\n",
    "            attempt += 1\n",
    "            try:\n",
    "                # url-ссылка указана внутри цикла, т.к. вне цикла будет использована вся ссылка и перебор параметров не пойдет\n",
    "                url = f'http://example/sale?BeginDate={date_start}&EndDate={date_end}'\n",
    "                headers = {\n",
    "                    'Authorization': f'Basic {credentials_b64}',\n",
    "                    'User-Agent': 'Python ETL Script'\n",
    "                }\n",
    "                \n",
    "                response = requests.get(\n",
    "                    url,\n",
    "                    headers=headers,\n",
    "                    verify=False  # только для теста!\n",
    "                )\n",
    "                if response.status_code == 200:\n",
    "                    df_new_2 = response.json()\n",
    "                    all_data_new_2.extend(df_new_2)\n",
    "                    logger.info(f\"✅ Получено: {len(df_new_2)} строк\")\n",
    "                    success = True  # ✅ Устанавливаем успех\n",
    "                else:\n",
    "                    print(f\"❌ Ошибка: {response.status_code}, {response.text[:200]}\")\n",
    "            except Exception as e:\n",
    "                    logger.error(f\"❌ Ошибка: {e}\")\n",
    "                    success = False  # ❌ Устанавливаем ошибку\n",
    "            \n",
    "        # повтор цикла если произошла ошибка \n",
    "            if not success and attempt < 3:\n",
    "                logger.info(\"Повтор через 5 сек...\")\n",
    "                time.sleep(5)\n",
    "        \n",
    "    # расчет времени окончания запроса (чанка)\n",
    "    duration = datetime.now() - start_time\n",
    "    logger.info(f\"✅ Готово за {duration.total_seconds():.1f} сек\")\n",
    "                \n",
    "duration_all = datetime.now() - start_time_all\n",
    "logger.info(f\"✅ Вся обработка:  {duration_all.total_seconds():.1f} сек\")\n",
    "\n",
    "# ✅ Теперь сразу обрабатываем\n",
    "if all_data_new_2:\n",
    "    df_new_2 = pd.DataFrame(all_data_new_2)\n",
    "        \n",
    "    df_new_2['date_doc'] = pd.to_datetime(df_new_2['date_doc'], errors='coerce')\n",
    "    \n",
    "\n",
    "        # заполняем пропуски (при первичном анализе выявлены пропуски в полях ниже)\n",
    "    future = pd.Timestamp('2099-12-31 23:59:59') # заглушка\n",
    "   df_new_2['date_doc'] = pd.to_datetime(df_new_2['date_doc'], errors='coerce')\n",
    "\n",
    "\n",
    "    logger.info(f\"✅ DataFrame создан: {len(df_new_2)} строк\")\n",
    "    \n",
    "else:\n",
    "    logger.info(\"❌ Нет данных для обработки!\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a5319f09-c70b-4f96-9225-37185d29c1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'исх.df_new сумма quantity: 15000.00'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'исх.df_new сумма price: 30000000.00'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'исх.df_new сумма revenue_no_nds: 40000000.00'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'НОВЫЙ.df_new сумма quantity: 30000.00'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'НОВЫЙ.df_new сумма price: 60000000.00'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'НОВЫЙ.df_new сумма revenue_no_nds: 80000000.00'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# симитируем изменение числовых параметров через х2, так проще отследить изменения в нашей БД по итогу обновления\n",
    "\n",
    "df_new_2['quantity'] = df_new_2['quantity']*2\n",
    "df_new_2['price'] = df_new_2['price']*2\n",
    "df_new_2['revenue_no_nds'] = df_new_2['revenue_no_nds']*2\n",
    "\n",
    "display(f\"исх.df_new сумма quantity: {df_new['quantity'].sum()}\")\n",
    "display(f\"исх.df_new сумма price: {df_new['price'].sum()}\")\n",
    "display(f\"исх.df_new сумма revenue_no_nds: {df_new['revenue_no_nds'].sum()}\")\n",
    "\n",
    "display(f\"НОВЫЙ.df_new сумма quantity: {df_new_2['quantity'].sum()}\")\n",
    "display(f\"НОВЫЙ.df_new сумма price: {df_new_2['price'].sum()}\")\n",
    "display(f\"НОВЫЙ.df_new сумма revenue_no_nds: {df_new_2['revenue_no_nds'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d05546-4d7d-46db-9347-fbc446af6ff2",
   "metadata": {},
   "source": [
    "наши числовые поля в сумме в 2 раза больше чем в исходном датафрейме"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a482f866-169d-4379-b0b8-292227458d98",
   "metadata": {},
   "source": [
    "#### присваиваем hash_id\n",
    "далее удалим дубликаты по hash_id на случай если получили дублирование при перекрытии чанков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "77077114-d7cd-4700-a8cd-46af3c1587cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_2['hash_id'] = df_new_2.astype(str).apply(''.join, axis=1).apply(\n",
    "        lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b4512643-c358-4699-ae49-195c5a6cf448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверяем количество уникальных id через хэш\n",
    "df_new_2['hash_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f57a5d15-e348-466e-b064-5e807ad2e9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'количество дубликатов: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nomenclature</th>\n",
       "      <th>buyer</th>\n",
       "      <th>sales_doc</th>\n",
       "      <th>date_doc</th>\n",
       "      <th>region</th>\n",
       "      <th>distribution_channel</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "      <th>revenue_no_nds</th>\n",
       "      <th>tariff</th>\n",
       "      <th>price_in_currency</th>\n",
       "      <th>revenue_in_currency</th>\n",
       "      <th>rate_currency</th>\n",
       "      <th>country</th>\n",
       "      <th>incoterms</th>\n",
       "      <th>hash_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nomenclature, buyer, sales_doc, date_doc, region, distribution_channel, quantity, price, revenue_no_nds, tariff, price_in_currency, revenue_in_currency, rate_currency, country, incoterms, hash_id]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:07:01,981 | INFO | ✅ Удалены дубликаты по hash_id\n"
     ]
    }
   ],
   "source": [
    "# проверяем количество дубликатов\n",
    "display(f\"количество дубликатов: {df_new_2.duplicated().sum()}\")\n",
    "display(df_new_2[df_new_2.duplicated()])\n",
    "# удаляем дубликаты\n",
    "df_new_2 = df_new_2.drop_duplicates(['hash_id'])\n",
    "# проверяем размер df\n",
    "display(len(df_new_2))\n",
    "logger.info(f\"✅ Удалены дубликаты по hash_id\")\n",
    "\n",
    "# дубликаты появляются из-за пограничного времени 00:00:00 в date_doc\n",
    "# в исходном отчете такие дубликаты отсутствуют"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da6bfe-23d5-405d-96fa-927e0ca34c4f",
   "metadata": {},
   "source": [
    "#### Проверяем пропуски, в т.ч. пустые значения ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "306c7bf3-2b53-4f6e-adee-43e108451370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Столбцы с пустыми строками: Index(['tariff', 'country', 'incoterms'], dtype='object')\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Проверяем наличие пустых строк, заполнены ''\n",
    "\n",
    "display(f\"Столбцы с пустыми строками: {df_new_2.columns[df_new_2.eq('').any()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5ff64ab5-db80-48a6-b8c2-f9983275c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем пустные значения, аналогично как с обработкой исторических данных\n",
    "df_new_2['tariff'] = df_new_2['tariff'].replace('', 0)\n",
    "df_new_2['country'] = df_new_2['country'].replace('', 'unknown')\n",
    "df_new_2['incoterms'] = df_new_2['incoterms'].replace('', 'unknown')\n",
    "\n",
    "# столбцы   ['country'], ['incoterms'] оста.тся без преобразований - обработка на уровне view БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "41fcd93a-9eff-4c1b-b758-e1a9d57607cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 442 entries, 0 to 441\n",
      "Data columns (total 16 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   nomenclature          442 non-null    object        \n",
      " 1   buyer                 442 non-null    object        \n",
      " 2   sales_doc             442 non-null    object        \n",
      " 3   date_doc              442 non-null    datetime64[ns]\n",
      " 4   region                442 non-null    object        \n",
      " 5   distribution_channel  442 non-null    object        \n",
      " 6   quantity              442 non-null    float64       \n",
      " 7   price                 442 non-null    float64       \n",
      " 8   revenue_no_nds        442 non-null    float64       \n",
      " 9   tariff                442 non-null    float64       \n",
      " 10  price_in_currency     442 non-null    float64       \n",
      " 11  revenue_in_currency   442 non-null    float64       \n",
      " 12  rate_currency         442 non-null    float64       \n",
      " 13  country               442 non-null    object        \n",
      " 14  incoterms             442 non-null    object        \n",
      " 15  hash_id               442 non-null    object        \n",
      "dtypes: datetime64[ns](1), float64(7), object(8)\n",
      "memory usage: 55.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_new_2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b293bb38-3fef-4168-b5e2-8858dead1402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:07:02,008 | INFO | ✅ Удалены дубликаты\n",
      "2025-09-30 17:07:02,009 | INFO | В таблице записей: 442\n"
     ]
    }
   ],
   "source": [
    "# удаляем дубликаты\n",
    "df_new_2 = df_new_2.drop_duplicates()\n",
    "logger.info(f\"✅ Удалены дубликаты\")\n",
    "logger.info(f\"В таблице записей: {len(df_new_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "721c7627-5032-4375-a077-fe200db47962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:07:02,019 | INFO | Уникальных unique_id: 442\n"
     ]
    }
   ],
   "source": [
    "df_new_2 = bisness_id(df_new_2)\n",
    "    # Проверяем стабильность\n",
    "logger.info(f\"Уникальных unique_id: {df_new_2['unique_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "20fab559-213e-4077-b86a-80f3718e28f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим время загрузки\n",
    "df_new_2['load_time'] = pd.Timestamp.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5cddb-7a63-47cd-9029-7b787c6cafa7",
   "metadata": {},
   "source": [
    "#### Инкрементальная загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "22c8ebf4-6051-4d06-96d3-9a4867cba54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:07:02,045 | INFO | ✅ Подключение к БД готово. Проверяем таблицу...\n"
     ]
    }
   ],
   "source": [
    "# Подключение в БД\n",
    "# Читаем пароль\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_port = os.getenv(\"DB_PORT\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "\n",
    "# Экранируем пароль\n",
    "encoded_password = quote_plus(db_password)\n",
    "\n",
    "# создаем ссылку на подключение\n",
    "db_url = f\"postgresql+psycopg2://{db_user}:{encoded_password}@{db_host}:{db_port}/{db_name}\"\n",
    "\n",
    "# Создаём движок для подключения по ссылке engine\n",
    "engine = create_engine(db_url, pool_pre_ping=True) \n",
    "\n",
    "logger.info(\"✅ Подключение к БД готово. Проверяем таблицу...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "15423c02-00cb-47cd-8c03-bcca44052cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:07:02,079 | INFO | 📊 В таблице dm.test_sale: 501 строк\n"
     ]
    }
   ],
   "source": [
    "# Проверка наличия таблицы и количества записей\n",
    "query_count_2 = \"\"\"\n",
    "SELECT COUNT(*) AS total_count \n",
    "FROM dm.test_sale;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    result_2 = pd.read_sql(query_count_2, engine)\n",
    "    logger.info(f\"📊 В таблице dm.test_sale: {result_2.iloc[0]['total_count']} строк\")\n",
    "except Exception as e:\n",
    "    logger.info(f\"❌ Ошибка при запросе к таблице: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f8503e0c-304a-4647-9dec-8ad0712c8552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:07:02,088 | INFO | ✅ Успешно загружено 501 уникальных ID из БД\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры ID из БД: ['....']\n"
     ]
    }
   ],
   "source": [
    "# Забираем все id из БД\n",
    "query_ids_2 = \"\"\"\n",
    "SELECT DISTINCT unique_id\n",
    "FROM dm.test_sale\n",
    "WHERE unique_id IS NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    existing_df_2 = pd.read_sql(query_ids_2, engine)\n",
    "    existing_df_2 = set(existing_df_2['unique_id'])\n",
    "    logger.info(f\"✅ Успешно загружено {len(existing_df_2)} уникальных ID из БД\")\n",
    "    # Покажем несколько первых\n",
    "    print(\"Примеры ID из БД:\", list(existing_df_2)[:5])\n",
    "except Exception as e:\n",
    "    logger.info(f\"❌ Не удалось загрузить unique_id: {e}\")\n",
    "    existing_df_2 = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0ae154c5-8ed6-4301-805e-a444d45bfab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:07:02,094 | INFO | ✅ В df_new найден столбец unique_id. Всего строк: 442\n",
      "2025-09-30 17:07:02,094 | INFO | 🔁 Уже есть в БД: 442\n",
      "2025-09-30 17:07:02,095 | INFO | 🆕 Новых ID: 0\n",
      "2025-09-30 17:07:02,097 | INFO | ✅ Отфильтровано для загрузки: 0 строк\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Проверим, есть ли unique_id в df\n",
    "if 'unique_id' not in df_new_2.columns:\n",
    "    logger.info(\"❌ Ошибка: в df нет столбца unique_id\")\n",
    "else:\n",
    "    logger.info(f\"✅ В df_new найден столбец unique_id. Всего строк: {len(df_new_2)}\")\n",
    "\n",
    "    # Преобразуем в set для быстрого сравнения\n",
    "    new_id_2 = set(df_new_2['unique_id'])\n",
    "\n",
    "    # Находим, сколько уже есть в БД\n",
    "    already_exists_2 = new_id_2 & existing_df_2 \n",
    "# Оператор & между двумя set в Python означает пересечение: возвращает новый set, содержащий только те элементы, которые присутствуют в обоих множествах.    \n",
    "     \n",
    "    brand_new_2 = new_id_2 - existing_df_2\n",
    "# возвращает новый set состящий из элементо отсутствующих в новом set\n",
    "\n",
    "    logger.info(f\"🔁 Уже есть в БД: {len(already_exists_2)}\")\n",
    "    logger.info(f\"🆕 Новых ID: {len(brand_new_2)}\")\n",
    "\n",
    "    # Фильтруем df — только новые строки\n",
    "    df_new_2_filtre = df_new_2[df_new_2['unique_id'].isin(brand_new_2)].copy()\n",
    "\n",
    "    logger.info(f\"✅ Отфильтровано для загрузки: {len(df_new_2_filtre)} строк\")\n",
    "    \n",
    "    # Проверим, нет ли дублей\n",
    "    if df_new_2_filtre['hash_id'].duplicated().any():\n",
    "        logger.info(\"⚠️ Найдены дубли по unique_id в новых данных!\")\n",
    "        df_new_2_filtre.drop_duplicates(subset=['unique_id'], keep='first', inplace=True)\n",
    "        logger.info(f\"✅ После удаления дублей: {len(df_new_2_filtre)} строк\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "71fee7c0-5320-4947-8108-fc373373c5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:07:02,106 | INFO | ✅ Успешно загружено 501 уникальных hash_id из БД\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры hash_id из БД: ['.....']\n"
     ]
    }
   ],
   "source": [
    "# Забираем все id из БД\n",
    "query_ids_hash_id_2 = \"\"\"\n",
    "SELECT DISTINCT hash_id\n",
    "FROM dm.test_sale\n",
    "WHERE unique_id IS NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    existing_df_hash_id_2 = pd.read_sql(query_ids_hash_id_2, engine)\n",
    "    existing_df_hash_id_2 = set(existing_df_hash_id_2['hash_id'])\n",
    "    logger.info(f\"✅ Успешно загружено {len(existing_df_hash_id_2)} уникальных hash_id из БД\")\n",
    "    # Покажем несколько первых\n",
    "    print(\"Примеры hash_id из БД:\", list(existing_df_hash_id_2)[:5])\n",
    "except Exception as e:\n",
    "    logger.info(f\"❌ Не удалось загрузить hash_id: {e}\")\n",
    "    existing_df_hash_id_2 = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "363d552c-1c1b-4c09-93ec-47a463d3ac3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:07:02,165 | INFO | ✅ В df_new_2 найден столбец hash_id. Всего строк: 442\n",
      "2025-09-30 17:07:02,166 | INFO | 🔁 Уже есть в БД: 0\n",
      "2025-09-30 17:07:02,167 | INFO | 🆕 Новых ID: 442\n",
      "2025-09-30 17:07:02,169 | INFO | ✅ Отфильтровано для загрузки: 442 строк\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Проверим, есть ли hash_id в df_new_2\n",
    "if 'hash_id' not in df_new_2.columns:\n",
    "    logger.info(\"❌ Ошибка: в df нет столбца hash_id\")\n",
    "else:\n",
    "    logger.info(f\"✅ В df_new_2 найден столбец hash_id. Всего строк: {len(df_new_2)}\")\n",
    "\n",
    "    # Преобразуем в set для быстрого сравнения\n",
    "    new_hash_id_2 = set(df_new_2['hash_id'])\n",
    "\n",
    "    # Находим, сколько уже есть в БД\n",
    "    already_exists_hash_id_2 = new_hash_id_2 & existing_df_hash_id_2 \n",
    "# Оператор & между двумя set в Python означает пересечение: возвращает новый set, содержащий только те элементы, которые присутствуют в обоих множествах.    \n",
    "     \n",
    "    brand_new_hash_id_2 = new_hash_id_2 - existing_df_hash_id_2\n",
    "# возвращает новый set состящий из элементо отсутствующих в новом set\n",
    "\n",
    "    logger.info(f\"🔁 Уже есть в БД: {len(already_exists_hash_id_2)}\")\n",
    "    logger.info(f\"🆕 Новых ID: {len(brand_new_hash_id_2)}\")\n",
    "\n",
    "    # Фильтруем df — только новые строки\n",
    "    df_new_hash_id_2 = df_new_2[df_new_2['hash_id'].isin(brand_new_hash_id_2)].copy()\n",
    "\n",
    "    logger.info(f\"✅ Отфильтровано для загрузки: {len(df_new_hash_id_2)} строк\")\n",
    "    \n",
    "    # Проверим, нет ли дублей\n",
    "    if df_new_hash_id_2['hash_id'].duplicated().any():\n",
    "        logger.info(\"⚠️ Найдены дубли по unique_id в новых данных!\")\n",
    "        df_new_hash_id_2.drop_duplicates(subset=['hash_id'], keep='first', inplace=True)\n",
    "        logger.info(f\"✅ После удаления дублей: {len(df_new_hash_id_2)} строк\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a5730-7036-433a-8d8f-d278842c2f0e",
   "metadata": {},
   "source": [
    "#### UPSERT через временную таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6cc890c5-b003-4c4e-accd-649d2f41e082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:07:02,579 | INFO | ✅ Данные во временной таблице\n"
     ]
    }
   ],
   "source": [
    "# Шаг 1: Создаём временную таблицу\n",
    "#df_new_2.to_sql(\n",
    "# передаем всю таблицу SQl сам разберется что обновить при измененном хэш и появлению новых unique_id\n",
    "df_new_2.to_sql(    \n",
    "    name='temp_sale',\n",
    "    con=engine,\n",
    "    schema='dm', # песочница\n",
    "    if_exists='replace',  # пересоздаём каждый раз\n",
    "    index=False,\n",
    "    method='multi',\n",
    "    chunksize=1000\n",
    ")\n",
    "\n",
    "logger.info(\"✅ Данные во временной таблице\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b19359ec-44a1-458a-899a-28b66305130a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:07:02,600 | INFO | ✅ UPSERT выполнен: 442 строк добавлено/обновлено\n"
     ]
    }
   ],
   "source": [
    "# Шаг 2: Выполняем UPSERT\n",
    "# задаем переменную обновления строк\n",
    "upsert_sql = \"\"\"\n",
    "    INSERT INTO dm.test_sale AS target\n",
    "    SELECT \n",
    "        nomenclature,\n",
    "        buyer,\n",
    "        sale,\n",
    "        date_doc,\n",
    "        region,\n",
    "        distribution_channel,\n",
    "        quantity, \n",
    "        price,\n",
    "        revenue_no_nds,\n",
    "        tariff,\n",
    "        price_in_currency,\n",
    "        revenue_in_currency,\n",
    "        rate_currency,\n",
    "        country,\n",
    "        incoterms,\n",
    "        hash_id,\n",
    "        bisiness_id,\n",
    "        duplicate_index,\n",
    "        unique_id,\n",
    "        load_time\n",
    "    FROM dm.temp_sale\n",
    "    ON CONFLICT (unique_id)\n",
    "    DO UPDATE SET\n",
    "        nomenclature = EXCLUDED.nomenclature,\n",
    "        buyer = EXCLUDED.buyer,\n",
    "        sale = EXCLUDED.sale,\n",
    "        date_doc = EXCLUDED.date_doc,\n",
    "        region = EXCLUDED.region,\n",
    "        distribution_channel = EXCLUDED.distribution_channel,\n",
    "        quantity = EXCLUDED.quantity,\n",
    "        price = EXCLUDED.price,\n",
    "        revenue_no_nds = EXCLUDED.revenue_no_nds,\n",
    "        tariff = EXCLUDED.tariff,\n",
    "        price_in_currency = EXCLUDED.price_in_currency,\n",
    "        revenue_in_currency = EXCLUDED.revenue_in_currency,\n",
    "        rate_currency = EXCLUDED.rate_currency,\n",
    "        country = EXCLUDED.country,\n",
    "        incoterms = EXCLUDED.incoterms,\n",
    "        hash_id = EXCLUDED.hash_id,\n",
    "        bisiness_id = EXCLUDED.bisiness_id,\n",
    "        duplicate_index = EXCLUDED.duplicate_index,\n",
    "        unique_id = EXCLUDED.unique_id,\n",
    "        load_time = EXCLUDED.load_time                \n",
    "    WHERE target.hash_id IS DISTINCT FROM EXCLUDED.hash_id;\n",
    "\"\"\"\n",
    "# Обновляет только если хэш (hash_id)  изменится\n",
    "\n",
    "\n",
    "# Выполняем в транзакции\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(upsert_sql))\n",
    "\n",
    "logger.info(f\"✅ UPSERT выполнен: {len(df_new_2)} строк добавлено/обновлено\")\n",
    "\n",
    "# Шаг 3: Удаляем временную таблицу\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"DROP TABLE IF EXISTS dm.temp_sale\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "24f685b1-446c-4b9b-bae0-209a84751237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:07:02,609 | INFO | 📊 Теперь в таблице: 501 строк\n"
     ]
    }
   ],
   "source": [
    "# Проверка сколько стало строк в БД\n",
    "query_count_after = \"\"\"\n",
    "SELECT COUNT(*) AS total_count \n",
    "FROM dm.test_sale;\n",
    "\"\"\"\n",
    "\n",
    "result_after = pd.read_sql(query_count_after, engine)\n",
    "logger.info(f\"📊 Теперь в таблице: {result_after.iloc[0]['total_count']} строк\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c76142ce-ca23-487a-b728-2274e02b9145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sum_quantity     sum_price  sum_revenue_no_nds\n",
      "0     40000.00  80000000.00     110000000.00\n"
     ]
    }
   ],
   "source": [
    "# Проверка сколько стало строк в БД\n",
    "query_count_after = \"\"\"\n",
    "SELECT \n",
    "    sum(quantity) as sum_quantity,\n",
    "    sum(price)  as sum_price,\n",
    "    sum(revenue_no_nds)  as sum_revenue_no_nds\n",
    "FROM dm.test_sale;\n",
    "\"\"\"\n",
    "\n",
    "result_after_2 = pd.read_sql(query_count_after, engine)\n",
    "print(result_after_2)\n",
    "#logger.info(f\"📊 Теперь в таблице: {result_after.iloc[0]['total_count']} строк\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7aa61efa-09da-4d31-b213-fb31ac0df216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sum_quantity     sum_price  sum_revenue_no_nds\n",
      "0      25000.00   50000000.00         70000000.00"
     ]
    }
   ],
   "source": [
    "# Проверка суммы по числовым полям  в БД\n",
    "query_count_after = \"\"\"\n",
    "SELECT \n",
    "    sum(quantity) as sum_quantity,\n",
    "    sum(price)  as sum_price,\n",
    "    sum(revenue_no_nds)  as sum_revenue_no_nds\n",
    "FROM dm.test_sale;\n",
    "\"\"\"\n",
    "\n",
    "result_after = pd.read_sql(query_count_after, engine)\n",
    "print(result_after)\n",
    "#logger.info(f\"📊 Теперь в таблице: {result_after.iloc[0]['total_count']} строк\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246bb21-5cc7-438a-92b2-64aa0480c0a7",
   "metadata": {},
   "source": [
    "Сравним итоговй результаты по числовым полям: как видно, разница на сумму которую добавили в df_new_2\n",
    "Инкрементальная загрузка данных прошла успешно.\n",
    "На данном этапе можем расширить диапазон для загрузки истоирческих данных и записать все в БД. На следующем этапе дорабатываем наш скрипт в части установления динамической даты: \n",
    "\n",
    "**Сегодняшняя дата (без времени)**\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "**Основной период: последние 30 дней**\n",
    "\n",
    "DateTo = today - timedelta(days=30)\n",
    "\n",
    "DateFrom = today\n",
    "\n",
    "в скрипте оставим только ячейки с парсингом, предобработкой данных и инкрементальной записью в БД\n",
    "Останется настроить запуск нашего скрипта в cron или в Airflow на пример ежедневно в 3:00 MSK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239172bb-be6e-465e-abe1-63a02a4c8084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
